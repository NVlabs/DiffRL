name: sac

params:
  algo:
    name: sac

  model:
    name: soft_actor_critic

  network:
    name: soft_actor_critic
    separate: True
    space:
      continuous:
    mlp:
      units: ${resolve_child:[64, 64],${env.sac.actor_critic_mlp},units}
      activation: elu

      initializer:
        name: default
      regularizer:
        name: None
    log_std_bounds: [-5, 2]

  load_checkpoint: False
  load_path: nn/${env.name}_sac.pth

  config:
    name: ${env.name}_sac
    env_name: ${env.name}
    multi_gpu: False
    normalize_input: True
    reward_shaper:
      scale_value: 1.0
    gamma: 0.99
    init_alpha: 0.2
    alpha_lr: 1e-4
    tau: 0.95
    actor_lr: ${resolve_child:5e-4,${env.sac},actor_lr}
    critic_lr: ${resolve_child:5e-4,${env.sac},critic_lr}
    critic_tau: 0.005
    lr_schedule: adaptive
    lr_threshold: 0.008
    kl_threshold: 0.008
    score_to_win: 200000000 # never stop training
    max_epochs: ${resolve_child:5000,${env.sac},max_epochs}
    num_steps_per_episode: 128
    save_best_after: ${resolve_child:100,${env.sac},save_best_after}
    save_frequency: ${resolve_child:400,${env.sac},save_frequency}
    num_actors: ${resolve_child:128,${env.sac},num_actors}
    batch_size: ${resolve_child:4096,${env.sac},batch_size}
    learnable_temperature: true
    num_seed_steps: 10
    num_warmup_steps: 10
    replay_buffer_size: 1000000

    player:
      games_num: ${resolve_child:24,${env.player},games_num}
      num_actors: ${resolve_child:3,${env.player},num_actors}
      determenistic: True
      print_stats: True
