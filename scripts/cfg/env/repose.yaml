defaults:
  - _self_
  - rewards:
      - action_penalty
      - object_pos_err
      - reach_bonus
      - rot_reward_delta

name: warp_repose_task

score_keys:
  - object_rot_err
  - object_pose_err
  - reach_bonus
  - action_penalty
  - net_energy

config:
  _target_: dmanip.envs.ReposeTask
  num_envs: 128
  episode_length: 250
  render: ${general.render}
  reward_params:
    action_penalty: ${env.rewards.action_penalty}
    object_pos_err:
      ${env.rewards.object_pos_err}
      # hand_joint_pos_err: ${task.rewards.hand_joint_pos_err}
    rot_reward_delta: ${env.rewards.rot_reward_delta}
    reach_bonus: ${env.rewards.reach_bonus}
  hand_type: ${hand:allegro}
  stochastic_init: true
  use_autograd: true
  use_graph_capture: true
  # use_graph_capture: ${eval:'("shac" not in "${alg.name}")'}
  no_grad: ???

shac:
  actor_lr: 3e-4
  critic_lr: 3e-4
  actor_mlp:
    units: [128, 64, 32]
    activation: elu
  critic_mlp:
    units: [64, 64]

ppo:
  max_epochs: 5000
  save_best_after: 100
  save_frequency: 400
  num_actors: 256
  minibatch_size: 2048
  steps_num: 32
  actor_mlp:
    units: [64, 64]

player:
  deterministic: true
  games_num: 100000
  print_stats: true

# Note SVG doesn't like floats so we use ints
svg:
  num_train_steps: 11000000 # 11M
  replay_buffer_capacity: 1000000
