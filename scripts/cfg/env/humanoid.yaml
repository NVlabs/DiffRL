name: df_humanoid
env_name: HumanoidEnv

config:
  _target_: dflex.envs.HumanoidEnv
  render: ${general.render}
  device: ${general.device}
  num_envs: 64
  no_grad: ??? # decided based on algorithm
  stochastic_init: True
  episode_length: 1000
  MM_caching_frequency: 48
  termination_height: 0.74
  action_penalty: -0.002
  joint_vel_obs_scaling: 0.1
  termination_tolerance: 0.1
  height_rew_scale: 10.0
  up_rew_scale: 0.1 # 10.0 produces better looking behaviour
  heading_rew_scale: 1.0

shac:
  actor_lr: 2e-3
  critic_lr: 5e-4
  max_epochs: 5000
  target_critic_alpha: 0.99
  actor_mlp:
    units: [400, 200, 100]
  critic_mlp:
    units: [400, 200, 100]

ppo:
  max_epochs: 50000
  lr: 5e-4
  minibatch_size: 32768
  num_actors: 4096
  horizon_length: 32
  save_frequency: 500
  save_best_after: 500
  actor_mlp:
    units: [400, 200, 100]

sac:
  max_epochs: 5000
  batch_size: 2048
  num_actors: 64
  save_frequency: 500
  save_best_after: 500
  actor_critic_mlp:
    units: [512, 256]

# Note SVG doesn't like floats so we use ints
svg:
  num_train_steps: 11000000 # 11M
  replay_buffer_capacity: 1000000
